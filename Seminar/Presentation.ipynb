{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "author: \"Alexander Fottner, Linus Lach\"\n",
        "bibliography: Quellen.bib\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: [solarized, custom.scss]\n",
        "    width: 1000\n",
        "    height: 600\n",
        "    transition: fade\n",
        "---"
      ],
      "id": "65f347fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Bayes Hilbert Space GANs** {.title-slide}\n",
        "\n",
        "<br>\n",
        "\n",
        "**Alexander Fottner, Linus Lach**\n",
        "\n",
        "\n",
        "```{css}\n",
        ".center h2 {\n",
        "  text-align: center;\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "![](BHSGANs.png){.center}\n",
        "\n",
        "\n",
        "::: {style=\"display:none;\"}\n",
        "(\n",
        "\n",
        "\\def\\bE{{\\mathbb{E}}}\n",
        "\n",
        "\\def\\bR{{\\mathbb{R}}}\n",
        "\n",
        "\\def\\cB{{\\mathcal{B}}}\n",
        "\n",
        "\\def\\rmd{{\\mathrm{d}}}\n",
        "\n",
        ")\n",
        ":::\n",
        "\n",
        "## Basic Assumptions\n",
        "\n",
        "Let $({\\mathbb{R}},{\\mathcal{B}},\\lambda)$ be a measure space\n",
        "\n",
        "-    $\\cB$ the borel $\\sigma$-field over $\\bR$ and\n",
        "-    $\\lambda$ some$\\sigma$-finite base measure <span style=\"color:#93a1a1;\">(usually Lebesgue measure)</span>.\n",
        "\n",
        "Let $\\mu,\\nu$ be two probability measures s.t. $\\mu,\\nu$ are absolutely continuous w.r.t. $\\lambda$\n",
        "\n",
        "\n",
        "## Radon-Nikodym\n",
        "::: {.theorem}\n",
        "  Let $(\\Omega,\\Sigma)$ be a measureable space and $\\lambda,\\mu$ be $\\sigma$-finite measures on $(\\Omega,\\Sigma)$ s.t. $\\mu$ is absolutely continuous w.r.t. $\\lambda$.\n",
        "  Then, there exists a $\\Sigma$-m.b. function $p_\\mu:X\\to[0,\\infty)$, s.t. for any m.b. set $A\\in\\Sigma$\n",
        "  $$\\mu(A) = \\int_A p_\\mu\\: \\rmd \\lambda.$$\n",
        ":::\n",
        "\n",
        "$p_\\mu$ is usually denoted by $\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\lambda}$ and called **Radon-Nikodym derivative**.\n",
        "\n",
        "\n",
        "## (Statistical) Divergences\n",
        "\n",
        "Allow for a quantification of dissimilarities between probability measures. \n",
        "\n",
        "**Kullback-Leibler (KL) Divergence**\n",
        "\n",
        "$$\n",
        " D_{\\mathrm{KL}}(\\mu,\\nu) = \\int_\\bR p_\\nu(x)\\log\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\lambda(\\rmd x).\n",
        "$$\n",
        "\n",
        "## (Statistical) Divergences\n",
        "\n",
        "**KL Divergence: Example**\n",
        "\n",
        "Let $\\lambda$ be the lebesgue measure on $\\bR$, $\\mu\\sim \\mathcal{N}(\\mu_1,\\sigma_1)$, and $\\nu\\sim\\mathcal{N}(\\mu_2,\\sigma_2)$. Then,\n",
        "\n",
        "\n",
        "```{=tex}\n",
        "\\begin{align*}\n",
        " D_{\\mathrm{KL}}(\\mu,\\nu) &= \\int_\\mathbb{R} p_\\nu(x)\\log\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\lambda(\\mathrm{d} x)\\\\\n",
        " &= ...\\\\\n",
        " &= \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "## $f$-divergences\n",
        "\n",
        "::: {.definition}\n",
        "Let $f:\\bR_+\\to\\bR$ be a lower-semicontinuous function, satisfying $f(1) = 0$. Then, the $f$-divergence with *generator* $f$ is defined as\n",
        "\n",
        "$$\n",
        "  D_{f}(\\mu,\\nu) = \\int_\\bR p_\\nu(x)f\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\lambda(\\mathrm{d}x).\n",
        "$$\n",
        ":::\n",
        "\n",
        "::: aside \n",
        "For the KL-divergence, $f$ is given by $f(x) = x\\cdot\\log(x)$.\n",
        ":::\n",
        "\n",
        "## Estimating $f$-divergences\n",
        "\n",
        "**Problem:** How to calculate divergence from finite samples if underlying distribution is unknown?\n",
        "\n",
        " . . . \n",
        "\n",
        "**Idea:** Find an easier to estimate, lower bound for $f$-divergence!\n",
        "\n",
        "## Estimating $f$-divergences\n",
        "\n",
        "[@nguyen2010estimating]\n",
        "\n",
        "For any $f$-divergence, it holds that \n",
        "\n",
        "$$\n",
        "D_f(\\mu,\\nu)  \\geq \\sup_{T\\in\\mathcal{T}}\\left\\{\\bE_\\mu (T) - \\bE_\\nu(f^*\\circ T)\\right\\}.\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "What is $\\mathcal{T}$ and $f^*$.\n",
        "\n",
        "\n",
        "## Bla\n",
        "\n",
        "## References\n",
        "\n",
        ":::{#refs}\n",
        ":::"
      ],
      "id": "bc0575a9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}