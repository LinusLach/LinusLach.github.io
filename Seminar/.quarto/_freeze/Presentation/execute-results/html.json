{
  "hash": "cdd91b068dd82c01fb557f050a3c6781",
  "result": {
    "markdown": "---\nauthor: \"Alexander Fottner, Linus Lach\"\nbibliography: Quellen.bib\nformat:\n  revealjs:\n    theme: [solarized, custom.scss]\n    width: 1000\n    height: 600\n    transition: fade\n---\n\n\n##  {.title-slide}\n\n![](BHSGANs.png){fig-align=\"center\"}\n\n## **Bayes Hilbert Space GANs**\n\n<br>\n\n**Alexander Fottner, Linus Lach**\n\n![](BHSGANs.png){fig-align=\"center\"}\n\n::: {style=\"display:none;\"}\n(\n\n\\def\\bE{{\\mathbb{E}}}\n\n\\def\\bR{{\\mathbb{R}}}\n\n\\def\\cB{{\\mathcal{B}}}\n\n\\def\\rmd{{\\mathrm{d}}}\n\n)\n:::\n\n## Basic Assumptions\n\nLet $({\\mathbb{R}},{\\mathcal{B}},\\lambda)$ be a measure space with\n\n-   ${\\mathcal{B}}$ the borel $\\sigma$-field over ${\\mathbb{R}}$ and\n-   $\\lambda$ some $\\sigma$-finite base measure [(usually Lebesgue measure)]{style=\"color:#93a1a1;\"}.\n\nLet $\\mu,\\nu$ be two probability measures s.t. $\\mu,\\nu$ are absolutely continuous w.r.t. $\\lambda$\n\n## Radon-Nikodym\n\n::: theorem\nLet $(\\Omega,\\Sigma)$ be a measureable space and $\\lambda,\\mu$ be $\\sigma$-finite measures on $(\\Omega,\\Sigma)$ s.t. $\\mu$ is absolutely continuous w.r.t. $\\lambda$. Then, there exists a $\\Sigma$-m.b. function $p_\\mu:X\\to[0,\\infty)$, s.t. for any m.b. set $A\\in\\Sigma$ $$\\mu(A) = \\int_A p_\\mu\\: {\\mathrm{d}}\\lambda.$$\n:::\n\n$p_\\mu$ is usually denoted by $\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\lambda}$ and called **Radon-Nikodym derivative**.\n\n## (Statistical) Divergences\n\nAllow for a quantification of dissimilarities between probability measures.\n\n**Kullback-Leibler (KL) Divergence**\n\n$$\n D_{\\mathrm{KL}}(\\mu,\\nu) = \\int_{\\mathbb{R}}p_\\nu(x)\\log\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\lambda({\\mathrm{d}}x).\n$$\n\n## (Statistical) Divergences\n\n**KL Divergence: Example**\n\nLet $\\lambda$ be the lebesgue measure on ${\\mathbb{R}}$, $\\mu\\sim \\mathcal{N}(\\mu_1,\\sigma_1)$, and $\\nu\\sim\\mathcal{N}(\\mu_2,\\sigma_2)$. Then,\n\n\n```{=tex}\n\\begin{align*}\n D_{\\mathrm{KL}}(\\mu,\\nu) &= \\int_\\mathbb{R} p_\\nu(x)\\log\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\lambda(\\mathrm{d} x)\\\\\n &= ...\\\\\n &= \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n\\end{align*}\n```\n\n## $f$-divergences\n\n::: definition\nLet $f:{\\mathbb{R}}_+\\to{\\mathbb{R}}$ be a convex, lower-semicontinuous function, satisfying $f(1) = 0$. Then, the $f$-divergence with *generator* $f$ is defined as\n\n$$\n  D_{f}(\\mu,\\nu) = \\int_{\\mathbb{R}}p_\\nu(x)f\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\lambda(\\mathrm{d}x).\n$$\n:::\n\n::: aside\nFor the KL-divergence, $f$ is given by $f(x) = x\\cdot\\log(x)$.\n:::\n\n## Estimating $f$-divergences\n\n**Problem:** How to calculate divergence from finite samples if the underlying distribution is unknown?\n\n. . .\n\n**Idea:** Find an easier to estimate, lower bound for $f$-divergence!\n\n## Estimating $f$-divergences\n\n[@nguyen2010estimating]\n\nFor any $f$-divergence, it holds that\n\n$$\nD_f(\\mu,\\nu)  \\geq \\sup_{T\\in\\mathcal{T}}\\left\\{{\\mathbb{E}}_\\mu (T) - {\\mathbb{E}}_\\nu(f^*\\circ T)\\right\\}.\n$$\n\n. . .\n\n**Now, what are** $f^*$ and $\\mathcal{T}$ ??\n\n## Fenchel conjugate\n\n[@touchette2005legendre] Define for an arbitrary function $f:{\\mathbb{R}}\\to{\\mathbb{R}}$\n\n$$\n f^*(y) := \\sup_{x\\in \\mathrm{dom}(f)}\\{xy-f(x)\\}.\n$$ Then, $f^*$ is called *Fenchel-* or *convex-conjugate* of $f$.\n\n## Fenchel conjugate: Example\n\nLet $x\\mapsto ax-b$ with $a,b\\in {\\mathbb{R}}$. Then\n\n\n```{=tex}\n\\begin{align*}\nf^*(y) &= \\sup_{x\\in \\mathrm{dom}(f)}\\{xy-f(x)\\}\\\\\n  &=b\\mathbb{I}_{\\{y=a\\}} + \\infty \\mathbb{I}_{\\{y\\neq a\\}}\n\\end{align*}\n```\n\n::: aside\nAssuming, that $0\\cdot(+\\infty) = 0$.\n:::\n\n## Fenchel Duality [@textbook]\n\n::: theorem\nLet $f:{\\mathbb{R}}\\to{\\mathbb{R}}$ be a convex function. If $f$ is lower semi-continuous, then $f^{**}(x) = f(x)$ for all $x\\in{\\mathbb{R}}$.\n:::\n\n. . .\n\nNote, that $$\n f^{**}(x) = \\sup_{y\\in\\mathrm{dom}(f^{*})}\\{xy-f^*(y)\\}.\n $$\n\n## Estimating $f$-divergences\n\n[@nguyen2010estimating]\n\nFor any $f$-divergence, it holds that\n\n::: {style=\"font-size:20pt\"}\n::: {.fragment .fade-out style=\"position:absolute;\" fragment-index=\"1\"}\n\n```{=tex}\n\\begin{align*}\nD_f(\\mu,\\nu)  \\geq \\sup_{T\\in\\mathcal{T}}\\left\\{{\\mathbb{E}}_\\mu (T) - {\\mathbb{E}}_\\nu(f^*\\circ T)\\right\\}.\\phantom{\\int}\\tag{4}\n\\end{align*}\n```\n\n:::\n\n::: {.fragment style=\"position:absolute;\" fragment-index=\"1\"}\n\n```{=tex}\n\\begin{align*}\nD_f(\\mu,\\nu) &=\\int_{\\mathbb{R}} p_\\nu(x)f\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right) \\lambda(\\mathrm{dx})\\tag{1}\\\\\n&=\\int_{\\mathbb{R}} \\sup_{t\\in\\mathrm{dom}(f^*)}\\left\\{t\\frac{p_\\mu(x)}{p_\\nu(x)}-f^*(t)\\right\\} \\nu(\\mathrm{dx})\\tag{2} \\\\\n &\\geq \\sup_{T\\in\\mathcal{T}} \\left(\\int_\\mathbb{R} T(x)\\mu(\\mathrm{d}x) - \\int_\\mathbb{R}f^*\\circ T(x)\\nu(\\mathrm{d}x)\\right)\\tag{3}\\\\\n &= \\sup_{T\\in\\mathcal{T}}\\left\\{{\\mathbb{E}}_\\mu (T) - {\\mathbb{E}}_\\nu(f^*\\circ T)\\right\\}.\\tag{4}\\\\\n\\end{align*}\n```\n\n:::\n:::\n\n## The set $\\mathcal{T}$\n\nBy now, we still haven't defined $\\mathcal{T}$.\n\n. . .\n\nIt turns out, that\n\n$$\n\\mathcal{T} = \\mathcal{T}_{f^*} := \\{T:{\\mathbb{R}}\\to\\mathrm{dom}(f^*)|T\\text{ is } {\\mathcal{B}}\\text{-measurable}\\}\n$$ is a valid choice!\n\n. . .\n\nSo...\n\n## \n\n![](Meme01.png){fig-align=\"center\"}\n\n## Estimating $f$-divergences: Conclusion\n\nBy using neural networks, we can effectively estimate\n\n$$\n\\sup_{T\\in\\mathcal{T}}\\left\\{{\\mathbb{E}}_\\mu (T) - {\\mathbb{E}}_\\nu(f^*\\circ T)\\right\\}. \n$$\n\n## Bayes spaces [@boogaart2010bayes]\n\n::: {style=\"font-size:24pt\"}\nRecall, for the base measure $\\lambda$, we define\n\n$$\nB(\\lambda) := M(\\lambda)/=_B,\n$$ where\n\n-   $M(\\lambda)$ set of all $\\sigma$-finite measures that are equivalent to $\\lambda$ and\n-   $\\mu=_B\\nu$ if there exists a constant $c\\geq 0$ s.t. $\\mu(A)=c\\nu(A)$ for all $A\\in{\\mathcal{B}}$.\n:::\n\n## Bayes linear spaces [@boogaart2010bayes]\n\nFor $\\mu,\\nu\\in B(\\lambda)$, define for $A\\in{\\mathcal{B}}$ and $\\alpha\\in{\\mathbb{R}}$\n\n\n```{=tex}\n\\begin{align*}\n  (\\mu \\oplus\\nu)(A) &:= \\int_A \\frac{\\mathrm{d} \\mu}{\\mathrm{d} \\lambda}\\frac{\\mathrm{d} \\nu}{\\mathrm{d} \\lambda}\\: \\mathrm{d}\\lambda\\\\\n  (\\mu \\ominus\\nu)(A) &:= \\int_A \\frac{\\mathrm{d} \\mu}{\\mathrm{d} \\nu}\\: \\mathrm{d}\\lambda\\\\\n  (\\alpha \\odot\\nu)(A) &:= \\int_A \\left(\\frac{\\mathrm{d} \\mu}{\\mathrm{d} \\lambda}\\right)^\\alpha \\: \\mathrm{d}\\lambda\\\\\n\\end{align*}\n```\n\n## $B^p$ space and $\\mathrm{clr}$[@van2014bayes]\n\nFor $p\\geq 1$ define\n\n$$\n  B^p(\\lambda) = \\left\\{\\mu\\in B(\\lambda): \\int_{\\mathbb{R}}\\left|\\log\\left(\\frac{{\\mathrm{d}}\\mu}{{\\mathrm{d}}\\lambda}\\right) \\right|^p{\\mathrm{d}}\\lambda <+\\infty\\right\\}.\n$$\n\nThe $\\mathrm{clr}$ of $\\mu\\in B^1(p)$ is defined as\n\n$$\n\\mathrm{clr}(\\mu) = \\log\\left(\\frac{{\\mathrm{d}}\\mu}{{\\mathrm{d}}\\lambda}\\right) - \\frac{1}{\\lambda(\\Omega)}\\int \\log\\left(\\frac{{\\mathrm{d}}\\mu}{{\\mathrm{d}}\\lambda}\\right)\\: {\\mathrm{d}}\\lambda.\n$$\n\n## Properties of $\\mathrm{clr}$[@van2014bayes]\n\n1.  $\\mathrm{clr}:B^2(\\lambda)\\to L^2_0(\\lambda)$\n2.  $\\mathrm{clr}$ is an isometric isomorphism between $B^2(\\lambda)$ and $L^2_0(\\lambda)$.\n3.  $\\mathrm{clr}$ can be used to construct an inner product and subsequently a hilbert space.\n\n## Bayes Hilbert Space\n\n::: {style=\"font-size:24pt\"}\nLet $\\lambda$ from now on be a probability measure and $p_\\mu$ and $p_\\nu$ densities in $B^2(\\lambda)$. Define\n:::\n\n::: {style=\"font-size:22pt\"}\n$$\n\\langle p_\\mu,p_\\nu \\rangle_{B^2(\\lambda)} := \\int_{\\mathbb{R}}\\mathrm{clr}(\\mu)(x)\\:\\mathrm{clr}(\\nu)(x)\\:\\lambda({\\mathrm{d}}x).\n$$\n:::\n\n::: {style=\"font-size:22pt\"}\nThen, [@van2014bayes] showed, that $$\\langle \\,\\cdot,\\cdot \\rangle_{B^2(\\lambda)}:B^2(\\lambda)\\times B^2(\\lambda) \\to {\\mathbb{R}}$$ is a well-defined inner product on $B^2(\\lambda)$ and the pair $(B^2(\\lambda),\\langle \\,\\cdot,\\cdot \\rangle_{B^2(\\lambda)})$ is a separable hilbert space called *Bayes Hilbert Space.*\n:::\n\n## Bayes Hilbert Metric\n\nThe map $$\nd_{B^2(\\lambda)}(p_\\mu,p_\\nu) := \\sqrt{\\langle \\,p_\\mu\\ominus p_\\nu,p_\\mu\\ominus p_\\nu \\rangle_{B^2(\\lambda)}}\n$$\n\nis a well-defined metric.\n\n## Bayes Hilbert Space GAN\n\n. . .\n\n![](PPAP.jpg){fig-align=\"center\"}\n\n## Fenchel conjugate for locally non-convex functions\n\n**Lemma:**\n\n::: {style=\"font-size:22pt;\"}\nLet $f:[0,\\infty)\\to{\\mathbb{R}}$ satisfy the following conditions:\n\n1.  $f$ is differentiable on $(0,\\infty)$,\n2.  $f$ is convex on $(0,\\infty)\\setminus (a,b)$, with $0\\leq a < b <\\infty$, and\n3.  $\\lim_{x\\searrow 0} f(x) <\\infty$.\n\nThen, the function $$\nf^*:\\mathrm{Im}(f^\\prime)\\to{\\mathbb{R}}\\cup\\{+\\infty\\},\\: y\\mapsto\\sup_{x\\in(0,\\infty)}\\{xy-f(x)\\}\n$$\n\nis well-defined and convex.\n:::\n\n## Example\n\n::: {style=\"font-size:20pt;\"}\n\n```{=tex}\n\\begin{align*}\n&f:[0,\\infty)\\to[0,\\infty)\\\\\n&x\\mapsto x\\cdot\\log(x)^2\\\\\n&\\\\\n&f^*:[-1,\\infty) \\to [0,\\infty),\\\\\n&y\\mapsto\n        \\begin{cases}\n                2\\left(-1 + \\sqrt{1+y}\\right)\\exp\\left(-1+\\sqrt{1+y}\\right), \\quad &y\\geq 0,\\\\\n                0, &y \\in [-1,0).\n        \\end{cases}\n\\end{align*}\n```\n\n:::\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Presentation_files/figure-revealjs/unnamed-chunk-2-1.png){fig-pos='center' width=1920}\n:::\n:::\n\n\n## What about $D_f(\\mu,\\nu)$?\n\n. . .\n\n**Lemma:**\n\nLet $f:[0,\\infty)\\to{\\mathbb{R}},\\, x\\mapsto x\\log(x)^2$. For any probability measures $\\mu,\\nu\\in B^2(\\lambda)$, $D_f(\\mu,\\nu)$ is well-defined in the sense that $D_f(\\mu,\\nu) \\geq 0$ and $D_f(\\mu,\\nu) = 0 \\iff \\mu=\\nu \\: \\lambda$-a.s.\n\n## But there is more!\n\n::: {style=\"font-size:24pt;\"}\n::: {.fragment .fade-out style=\"position:absolute;\" fragment-index=\"1\"}\n\n```{=tex}\n\\begin{align*}\nD_f(\\mu,\\nu) &=\\phantom{\\int} d_{B^2(\\lambda)}(p_\\mu,p_\\nu)^2 + \\mathbb{E}_\\mu\\left(\\log(\\mu\\ominus \\nu)\\right)^2. \n\\end{align*}\n```\n\n:::\n\n::: {.fragment style=\"position:absolute;\" fragment-index=\"1\"}\n\n```{=tex}\n\\begin{align*}\n D_f(\\mu,\\nu)   &=\\int_{[0,\\infty)} f\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)\\nu(\\mathrm{d}x) \\\\\n                 &\\geq \\int_{[0,\\infty)} \\sup_{x\\in\\mathrm{dom}(f^*)}\\left\\{x\\frac{p_\\mu(x)}{p_\\nu(x)}-f^*(x)\\right\\}\\nu(\\mathrm{d} x)\\nonumber\\\\\n                 &\\geq \\sup_{T\\in\\mathcal{T}} \\left(\\int_{[0,\\infty)} T\\:\\mathrm{d}\\mu-\\int_{[0,\\infty)} f^*\\circ T\\:\\mathrm{d} \\nu \\right)\\nonumber\\\\\n                 &= \\sup_{T\\in\\mathcal{T}}\\bigg\\{\\mathbb{E}_\\mu(T)- \\mathbb{E}_\\nu(f^*\\circ T)\\bigg\\}.\n\\end{align*}\n```\n\n:::\n:::\n\n## Optimizing for $T$\n\n::: {style=\"font-size:24pt;\"}\n**Theorem:**\n\nLet $f:[0,\\infty)\\to{\\mathbb{R}},\\: x\\mapsto x\\log(x)^2$ and $\\mu,\\nu\\in B^2(\\lambda)$. Then, $T^*(x) :=f^\\prime\\left(\\frac{p_\\mu(x)}{p_\\nu(x)}\\right)$ is an optimizer for $$\n    \\sup_{T\\in\\mathcal{T}}\\left\\{{\\mathbb{E}}_\\mu(T) - {\\mathbb{E}}_\\nu(f^*\\circ T) \\right\\}\n$$ with $$\n\\mathcal{T} = \\{T:{\\mathbb{R}}\\to(0,\\infty)|T\\text{ is } {\\mathcal{B}}\\text{-measurable}\\}.\n$$\n:::\n\n## Conclusion\n\nSince an optimal solution to the optimization problem\n\n$$\n    \\sup_{T\\in\\mathcal{T}}\\left\\{{\\mathbb{E}}_\\mu(T) - {\\mathbb{E}}_\\nu(f^*\\circ T) \\right\\}.\n$$ in terms of $T$ can be found analytically, we can leverage neural networks to approximate this solution.\n\n## Proof of work\n\n![](pow.png){fig-align=\"center\"}\n\n## \n\n![](Spoderman.jpg){fig-align=\"center\"}\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "Presentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}